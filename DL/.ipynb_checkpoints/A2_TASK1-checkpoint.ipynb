{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference pytorch\n",
    "\n",
    "np.arange(3.0)\n",
    "\n",
    "x = torch.empty(5,3) # will have some junk\n",
    "print(x[0,0])\n",
    "\n",
    "torch.zeros_like(x)\n",
    "torch.zeros(5,3)\n",
    "\n",
    "y = torch.rand(5,3)\n",
    "print(y)\n",
    "\n",
    "x.size()\n",
    "\n",
    "torch.tensor([2,1,4])\n",
    "\n",
    "torch.add(x,y)\n",
    "\n",
    "z = torch.tensor([1.234])\n",
    "z.item() #will work for one item to extract out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# way\n",
    "-> Took whole file as single dim for ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forest', 'mountain', 'coast', 'insidecity', 'street']\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "\n",
    "path = '11_ds_1/'\n",
    "class_names = os.listdir(path)\n",
    "print(class_names)\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for cn in range(len(class_names)):\n",
    "    for fname in os.listdir(path+class_names[cn]+'/'):\n",
    "        data.append(np.loadtxt(path+class_names[cn]+'/'+fname).flatten())\n",
    "        target.append(int(cn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "dim = 500\n",
    "h1_dim = 40\n",
    "h2_dim = 28\n",
    "out_dim = len(class_names)\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_epochs = 500\n",
    "batch_size = 32  # general rule is to try 32, 64, 128 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "\n",
    "train_data, test_data, train_target, test_target = train_test_split(data,target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "pca = decomposition.PCA(n_components = dim)\n",
    "pca.fit(train_data)\n",
    "print(np.shape(train_data))\n",
    "train_data = pca.transform(train_data)\n",
    "test_data = pca.transform(test_data)\n",
    "print(np.shape(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN model\n",
    "\n",
    "class Nural_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nural_net, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim,out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fun and update method\n",
    "\n",
    "tensor_train_data = torch.Tensor(train_data)\n",
    "tensor_train_target = torch.LongTensor(train_target) # long otherwise takes as float\n",
    " \n",
    "tensor_test_data = torch.Tensor(test_data)\n",
    "tensor_test_target = torch.LongTensor(test_target)\n",
    "\n",
    "# num_workers - how many parallel data loadings\n",
    "train_dataset = torch.utils.data.TensorDataset(tensor_train_data,tensor_train_target)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(tensor_test_data,tensor_test_target)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=4)\n",
    "\n",
    "net = Nural_net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# ASDG for generalised delta ig\n",
    "# schochastic gradient descent with nestrov momentum\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# optimizer = optim.Adagrad(net.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "# model1 is saved after fine tuning\n",
    "\n",
    "saved_net = './net_model2.pth'\n",
    "torch.save(net.state_dict(),saved_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive saved net\n",
    "\n",
    "test_net = Nural_net()\n",
    "test_net.load_state_dict(torch.load(saved_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# train data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = test_net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('train accuracy',end=':')\n",
    "print(correct/total)\n",
    "\n",
    "#test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = test_net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('test accuracy',end=':')\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 828\n",
    "h1_dim = 1000\n",
    "h2_dim = 500\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(data,target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AANN\n",
    "\n",
    "class AANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim,h1_dim)\n",
    "        self.fc4 = nn.Linear(h1_dim,dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = nn.Tanh()\n",
    "        x = self.fc1(x)\n",
    "        y = m(self.fc2(x))\n",
    "        x = m(self.fc3(y))\n",
    "        x = self.fc4(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for AANN\n",
    "\n",
    "tensor_data = torch.Tensor(train_data)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(tensor_data,tensor_data)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "aann = AANN()\n",
    "criterion = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adagrad(aann.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.11971286973428158\n",
      "100 0.06350918314464035\n",
      "150 0.044045921814228804\n",
      "200 0.033982196533609\n",
      "250 0.027916920326117957\n",
      "300 0.023932549482477562\n",
      "350 0.021223435070188274\n",
      "400 0.019314243980639037\n",
      "450 0.01789833396894946\n",
      "500 0.016800606788269112\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(loader):\n",
    "        \n",
    "        inputs, same_inputs = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = aann(inputs)\n",
    "        loss = criterion(outputs,same_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:0.5150147300493808\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('error',end=':')\n",
    "print(error/(num*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_net = './aann_model2.pth'\n",
    "torch.save(aann.state_dict(),saved_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aann_train = torch.Tensor(train_data)\n",
    "aann_test = torch.Tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1329, 828])\n",
      "torch.Size([1329, 500])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(aann_train.size())\n",
    "    _, aann_train = aann(aann_train)\n",
    "    _, aann_test = aann(aann_test)\n",
    "    print(aann_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "dim = 500\n",
    "h1_dim = 40\n",
    "h2_dim = 28\n",
    "out_dim = len(class_names)\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_epochs = 1000\n",
    "batch_size = 32  # general rule is to try 32, 64, 128 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fun and update method\n",
    "\n",
    "tensor_train_target = torch.LongTensor(train_target) # long otherwise takes as float\n",
    "\n",
    "tensor_test_target = torch.LongTensor(test_target)\n",
    "\n",
    "# num_workers - how many parallel data loadings\n",
    "train_dataset = torch.utils.data.TensorDataset(aann_train,tensor_train_target)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(aann_test,tensor_test_target)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=4)\n",
    "\n",
    "net2 = Nural_net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# ASDG for generalised delta ig\n",
    "# schochastic gradient descent with nestrov momentum\n",
    "optimizer = optim.SGD(net2.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# optimizer = optim.Adagrad(net.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1.606292659952527\n",
      "100 1.5998284238293057\n",
      "150 1.5855349958510625\n",
      "200 1.5318154136339823\n",
      "250 1.349795159385318\n",
      "300 1.0957818140586217\n",
      "350 0.9109914341710863\n",
      "400 0.7966734363209633\n",
      "450 0.6984166849794842\n",
      "500 0.6219683773460842\n",
      "550 0.569650000844683\n",
      "600 0.5284462880094846\n",
      "650 0.4911547010214556\n",
      "700 0.4523248037624927\n",
      "750 0.4126763663831211\n",
      "800 0.3723425723860661\n",
      "850 0.3316884047750916\n",
      "900 0.2934903258581956\n",
      "950 0.2577098155553852\n",
      "1000 0.2250289697909639\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net2(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:0.9533483822422875\n",
      "test accuracy:0.6306306306306306\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "# train data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('train accuracy',end=':')\n",
    "print(correct/total)\n",
    "\n",
    "#test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('test accuracy',end=':')\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_net = './net2_model2.pth'\n",
    "torch.save(net2.state_dict(),saved_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
