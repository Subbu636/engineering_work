{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference pytorch\n",
    "\n",
    "np.arange(3.0)\n",
    "\n",
    "x = torch.empty(5,3) # will have some junk\n",
    "print(x[0,0])\n",
    "\n",
    "torch.zeros_like(x)\n",
    "torch.zeros(5,3)\n",
    "\n",
    "y = torch.rand(5,3)\n",
    "print(y)\n",
    "\n",
    "x.size()\n",
    "\n",
    "torch.tensor([2,1,4])\n",
    "\n",
    "torch.add(x,y)\n",
    "\n",
    "z = torch.tensor([1.234])\n",
    "z.item() #will work for one item to extract out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NEVER RUN THIS\n",
    "\"\"\"\n",
    "\n",
    "# input data 1\n",
    "assert(1==0)\n",
    "\n",
    "path = '11_ds_1/'\n",
    "class_names = os.listdir(path)\n",
    "print(class_names)\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for cn in range(len(class_names)):\n",
    "    for fname in os.listdir(path+class_names[cn]+'/'):\n",
    "        data.append(np.loadtxt(path+class_names[cn]+'/'+fname).flatten())\n",
    "        target.append(int(cn)) \n",
    "print(np.shape(data))\n",
    "        \n",
    "# test train split\n",
    "\n",
    "train_data, test_data, train_target, test_target = train_test_split(data,target,test_size=0.2)\n",
    "np.save(path+'class_names.npy', class_names)\n",
    "np.save(path+'train_data.npy', train_data)\n",
    "np.save(path+'test_data.npy', test_data)\n",
    "np.save(path+'test_target.npy', test_target)\n",
    "np.save(path+'train_target.npy', train_target)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NEVER RUN THIS\n",
    "\"\"\"\n",
    "\n",
    "# input data 2\n",
    "assert(1==0)\n",
    "\n",
    "path = '11_ds_2/'\n",
    "class_names = os.listdir(path)\n",
    "print(class_names)\n",
    "data = []\n",
    "target = []\n",
    "for cn in range(len(class_names)):\n",
    "    data.append(np.genfromtxt(path+class_names[cn], delimiter=','))\n",
    "s = np.shape(data)\n",
    "print(s)\n",
    "for i in range(s[0]):\n",
    "    for j in range(s[1]):\n",
    "        target.append(i)\n",
    "data = np.reshape(data,(s[0]*s[1],s[2]))\n",
    "print(np.shape(data),np.shape(target))\n",
    "\n",
    "# test train split\n",
    "\n",
    "train_data, test_data, train_target, test_target = train_test_split(data,target,test_size=0.2)\n",
    "np.save(path+'class_names.npy', class_names)\n",
    "np.save(path+'train_data.npy', train_data)\n",
    "np.save(path+'test_data.npy', test_data)\n",
    "np.save(path+'test_target.npy', test_target)\n",
    "np.save(path+'train_target.npy', train_target)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Notes\n",
    "    1. Took whole file as single dim for ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1329, 828) (333, 828)\n",
      "['coast' 'forest' 'insidecity' 'mountain' 'street']\n"
     ]
    }
   ],
   "source": [
    "dir_path = '11_ds_1/'\n",
    "class_names = np.load(dir_path+'class_names.npy')\n",
    "train_data = np.load(dir_path+'train_data.npy')\n",
    "train_target = np.load(dir_path+'train_target.npy')\n",
    "test_data = np.load(dir_path+'test_data.npy')\n",
    "test_target = np.load(dir_path+'test_target.npy')\n",
    "print(np.shape(train_data),np.shape(test_data))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "dim = 500\n",
    "h1_dim = 50\n",
    "out_dim = len(class_names)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 600\n",
    "momentum = 0.9\n",
    "batch_size = 32  # general rule is to try 32, 64, 128 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1329, 828)\n",
      "(1329, 500)\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "\n",
    "pca = decomposition.PCA(n_components = 500)\n",
    "pca.fit(train_data)\n",
    "print(np.shape(train_data))\n",
    "train_data_pca = pca.transform(train_data)\n",
    "test_data_pca = pca.transform(test_data)\n",
    "print(np.shape(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFFNN model\n",
    "\n",
    "class MLFFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLFFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim) # linear -> fully connected layers\n",
    "        self.fc2 = nn.Linear(h1_dim,out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fun and update method\n",
    "\n",
    "tensor_train_data = torch.Tensor(train_data_pca)\n",
    "tensor_train_target = torch.LongTensor(train_target) # long otherwise takes as float\n",
    " \n",
    "tensor_test_data = torch.Tensor(test_data_pca)\n",
    "tensor_test_target = torch.LongTensor(test_target)\n",
    "\n",
    "# num_workers - how many parallel data loadings\n",
    "train_dataset = torch.utils.data.TensorDataset(tensor_train_data,tensor_train_target)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(tensor_test_data,tensor_test_target)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "net = MLFFNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# ASDG for generalised delta ig\n",
    "# schochastic gradient descent with nestrov momentum\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# optimizer = optim.Adagrad(net.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1.107709253515516\n",
      "100 0.6830926092962424\n",
      "150 0.5093000791825\n",
      "200 0.41182342586063203\n",
      "250 0.3457612983350243\n",
      "300 0.2953531828345287\n",
      "350 0.2538541241415909\n",
      "400 0.21813349328225567\n",
      "450 0.1873619727063037\n",
      "500 0.16065591363679796\n",
      "550 0.13775483613035508\n",
      "600 0.11814552073677381\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive saved net\n",
    "\n",
    "net = MLFFNN()\n",
    "net.load_state_dict(torch.load('net1_task1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:0.9932279909706546\n",
      "test accuracy:0.7027027027027027\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "# train data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('train accuracy',end=':')\n",
    "print(correct/total)\n",
    "\n",
    "#test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('test accuracy',end=':')\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "# model1 is saved after fine tuning\n",
    "\n",
    "saved_net_path = 'net1_task1.pth'\n",
    "torch.save(net.state_dict(),saved_net_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 828\n",
    "h1_dim = 1000\n",
    "h2_dim = 800\n",
    "h3_dim = 500\n",
    "h4_dim = 900\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 800\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AANN\n",
    "\n",
    "class AANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fc5 = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc3 = nn.Linear(h3_dim,h4_dim)\n",
    "        self.fc4 = nn.Linear(h4_dim,dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = nn.Tanh()\n",
    "        x = self.fc1(x)\n",
    "        x = m(self.fc2(x))\n",
    "        y = self.fc5(x)\n",
    "        x = m(self.fc3(y))\n",
    "        x = self.fc4(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for AANN\n",
    "\n",
    "tensor_data = torch.Tensor(train_data)\n",
    "tensor_data_test = torch.Tensor(test_data)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(tensor_data,tensor_data)\n",
    "dataset_test = torch.utils.data.TensorDataset(tensor_data_test,tensor_data_test)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "\n",
    "aann = AANN()\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adagrad(aann.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.15177173100057104\n",
      "100 0.09829308004606338\n",
      "150 0.07371671600001199\n",
      "200 0.05912337257038979\n",
      "250 0.049544438833282106\n",
      "300 0.04272164853201026\n",
      "350 0.03761483048754079\n",
      "400 0.033675467130683714\n",
      "450 0.030534732834923833\n",
      "500 0.027969556527123563\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(loader):\n",
    "        \n",
    "        inputs, same_inputs = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = aann(inputs)\n",
    "        loss = criterion(outputs,same_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aann = AANN()\n",
    "net.load_state_dict(torch.load('aann_task1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error:1.6991542116757066\n",
      "Test error:2.223556340412625\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Train error',end=':')\n",
    "print(error/(num*dim))\n",
    "\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader_test:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Test error',end=':')\n",
    "print(error/(num*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(aann.state_dict(),'aann_task1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aann_train = torch.Tensor(train_data)\n",
    "aann_test = torch.Tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1329, 828])\n",
      "torch.Size([1329, 500])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(aann_train.size())\n",
    "    _, aann_train = aann(aann_train)\n",
    "    _, aann_test = aann(aann_test)\n",
    "    print(aann_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "dim = 500\n",
    "h1_dim = 50\n",
    "out_dim = len(class_names)\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_workers = 8\n",
    "num_epochs = 1000\n",
    "batch_size = 32  # general rule is to try 32, 64, 128 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fun and update method\n",
    "\n",
    "tensor_train_target = torch.LongTensor(train_target) # long otherwise takes as float\n",
    "\n",
    "tensor_test_target = torch.LongTensor(test_target)\n",
    "\n",
    "# num_workers - how many parallel data loadings\n",
    "train_dataset = torch.utils.data.TensorDataset(aann_train,tensor_train_target)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(aann_test,tensor_test_target)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=num_workers)\n",
    "\n",
    "net2 = MLFFNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# ASDG for generalised delta ig\n",
    "# schochastic gradient descent with nestrov momentum\n",
    "optimizer = optim.SGD(net2.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# optimizer = optim.Adagrad(net.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net2(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = MLFFNN()\n",
    "net2.load_state_dict(torch.load('net2_task1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# train data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('train accuracy',end=':')\n",
    "print(correct/total)\n",
    "\n",
    "#test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('test accuracy',end=':')\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net2.state_dict(),'net2_task1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
