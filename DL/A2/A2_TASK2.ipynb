{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coast', 'forest', 'insidecity', 'mountain', 'street']\n",
      "(1662, 828)\n"
     ]
    }
   ],
   "source": [
    "dir_path = '11_ds_1/'\n",
    "class_names = np.load(dir_path+'class_names.npy')\n",
    "train_data = np.load(dir_path+'train_data.npy')\n",
    "train_target = np.load(dir_path+'train_target.npy')\n",
    "test_data = np.load(dir_path+'test_data.npy')\n",
    "test_target = np.load(dir_path+'test_target.npy')\n",
    "print(np.shape(train_data),np.shape(test_data))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AANN1\n",
    "\n",
    "dim = 828\n",
    "h1_dim = 1000\n",
    "h2_dim = 800\n",
    "h3_dim = 600\n",
    "h4_dim = 900\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "\n",
    "class AANN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AANN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fcl = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc3 = nn.Linear(h3_dim,h4_dim)\n",
    "        self.fc4 = nn.Linear(h4_dim,dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = nn.Tanh()\n",
    "        x = self.fc1(x)\n",
    "        x = m(self.fc2(x))\n",
    "        y = self.fcl(x)\n",
    "        x = m(self.fc3(y))\n",
    "        x = self.fc4(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for AANN1\n",
    "\n",
    "# loading data\n",
    "tensor_data = torch.Tensor(train_data)\n",
    "tensor_data_test = torch.Tensor(test_data)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(tensor_data,tensor_data)\n",
    "dataset_test = torch.utils.data.TensorDataset(tensor_data_test,tensor_data_test)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "\n",
    "aann1 = AANN1()\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "optimizer = optim.Adagrad(aann1.parameters(), lr=learning_rate,lr_decay=0)\n",
    "\n",
    "# training\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(loader):\n",
    "        \n",
    "        inputs, same_inputs = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = aann1(inputs)\n",
    "        loss = criterion(outputs,same_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "\n",
    "# computing error\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann1(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Train error',end=':')\n",
    "print(error/(num*dim))\n",
    "\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader_test:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann1(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Test error',end=':')\n",
    "print(error/(num*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(aann1.state_dict(),'aann1_task2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AANN2\n",
    "\n",
    "dim = h3_dim # from previous, AANN1\n",
    "h1_dim = 800\n",
    "h2_dim = 500\n",
    "h3_dim = 300\n",
    "h4_dim = 500\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "\n",
    "class AANN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AANN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fcl = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc3 = nn.Linear(h3_dim,h4_dim)\n",
    "        self.fc4 = nn.Linear(h4_dim,dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = nn.Tanh()\n",
    "        x = self.fc1(x)\n",
    "        x = m(self.fc2(x))\n",
    "        y = self.fcl(x)\n",
    "        x = m(self.fc3(y))\n",
    "        x = self.fc4(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for AANN2\n",
    "\n",
    "# loading data\n",
    "_, tensor_data = aann1(torch.Tensor(train_data))\n",
    "_, tensor_data_test = aann1(torch.Tensor(test_data))\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(tensor_data,tensor_data)\n",
    "dataset_test = torch.utils.data.TensorDataset(tensor_data_test,tensor_data_test)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "\n",
    "aann2 = AANN2()\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "optimizer = optim.Adagrad(aann2.parameters(), lr=learning_rate,lr_decay=0)\n",
    "\n",
    "# training\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(loader):\n",
    "        \n",
    "        inputs, same_inputs = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = aann2(inputs)\n",
    "        loss = criterion(outputs,same_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "\n",
    "# computing error\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann2(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Train error',end=':')\n",
    "print(error/(num*dim))\n",
    "\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader_test:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann2(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Test error',end=':')\n",
    "print(error/(num*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(aann2.state_dict(),'aann2_task2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aann_train = torch.Tensor(train_data)\n",
    "aann_test = torch.Tensor(test_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(aann_train.size())\n",
    "    _, aann_train = aann1(aann_train)\n",
    "    _, aann_train = aann2(aann_train)\n",
    "    _, aann_test = aann1(aann_test)\n",
    "    _, aann_test = aann2(aann_test)\n",
    "    print(aann_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "dim = h3_dim # from output of aann2\n",
    "h1_dim = 40\n",
    "h2_dim = 28\n",
    "out_dim = len(class_names)\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_epochs = 1000\n",
    "batch_size = 32  # general rule is to try 32, 64, 128 ... \n",
    "\n",
    "# DNN model developn \n",
    "\n",
    "class Nural_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nural_net, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim) \n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim,out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fun and update method\n",
    "\n",
    "tensor_train_target = torch.LongTensor(train_target) # long otherwise takes as float\n",
    "\n",
    "tensor_test_target = torch.LongTensor(test_target)\n",
    "\n",
    "# num_workers - how many parallel data loadings\n",
    "train_dataset = torch.utils.data.TensorDataset(aann_train,tensor_train_target)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(aann_test,tensor_test_target)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "net2 = Nural_net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# ASDG for generalised delta ig\n",
    "# schochastic gradient descent with nestrov momentum\n",
    "optimizer = optim.SGD(net2.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# optimizer = optim.Adagrad(net.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net2(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# train data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('train accuracy',end=':')\n",
    "print(correct/total)\n",
    "\n",
    "#test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('test accuracy',end=':')\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net2.state_dict(),'net_task2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
