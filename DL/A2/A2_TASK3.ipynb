{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from confusion_matrix import ConfusionPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '11_ds_2/'\n",
    "class_names = np.load(dir_path+'class_names.npy')\n",
    "orig_train_data = np.load(dir_path+'train_data.npy')\n",
    "train_target = np.load(dir_path+'train_target.npy')\n",
    "orig_test_data = np.load(dir_path+'test_data.npy')\n",
    "test_target = np.load(dir_path+'test_target.npy')\n",
    "print(np.shape(orig_train_data),np.shape(orig_test_data))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "print(np.max(orig_train_data))\n",
    "train_data = orig_train_data\n",
    "test_data = orig_test_data\n",
    "print(np.max(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data = np.reshape(train_data[0],(28,28))\n",
    "plt.imshow(show_data,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AANN1\n",
    "\n",
    "dim = 784\n",
    "h1_dim = 1000\n",
    "h2_dim = 700\n",
    "h3_dim = 600\n",
    "h4_dim = 700\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "num_workers = 8\n",
    "\n",
    "class AANN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AANN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fcl = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc3 = nn.Linear(h3_dim,h4_dim)\n",
    "        self.fc4 = nn.Linear(h4_dim,dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = nn.Softplus()\n",
    "        x = self.fc1(x)\n",
    "        x = m(self.fc2(x))\n",
    "        y = self.fcl(x)\n",
    "        x = m(self.fc3(y))\n",
    "        x = self.fc4(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = torch.Tensor(train_data)\n",
    "tensor_data_test = torch.Tensor(test_data)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(tensor_data,tensor_data)\n",
    "dataset_test = torch.utils.data.TensorDataset(tensor_data_test,tensor_data_test)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training for AANN1\n",
    "\n",
    "# loading data\n",
    "\n",
    "#aann1 = AANN1()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(aann1.parameters(), lr=learning_rate,lr_decay=0)\n",
    "\n",
    "# training\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(loader):\n",
    "        \n",
    "        inputs, same_inputs = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = aann1(inputs)\n",
    "        loss = criterion(outputs,same_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0 or epoch == 0.0):\n",
    "        print(epoch+1, running_loss/(count))\n",
    "        running_loss = 0.0\n",
    "        count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aann1 = AANN1()\n",
    "aann1.load_state_dict(torch.load('aann1_task3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing error\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann1(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Train error',end=':')\n",
    "print(error/(num*dim))\n",
    "\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader_test:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann1(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Test error',end=':')\n",
    "print(error/(num*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(aann1.state_dict(),'aann1_task3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AANN2\n",
    "\n",
    "dim = 600 # from previous, AANN1, 600\n",
    "h1_dim = 800\n",
    "h2_dim = 600\n",
    "h3_dim = 400\n",
    "h4_dim = 600\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "num_workers = 8\n",
    "\n",
    "class AANN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AANN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fcl = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc3 = nn.Linear(h3_dim,h4_dim)\n",
    "        self.fc4 = nn.Linear(h4_dim,dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = nn.Tanh()\n",
    "        x = self.fc1(x)\n",
    "        x = m(self.fc2(x))\n",
    "        y = self.fcl(x)\n",
    "        x = m(self.fc3(y))\n",
    "        x = self.fc4(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for AANN2\n",
    "\n",
    "aann_train = torch.Tensor(train_data)\n",
    "aann_test = torch.Tensor(test_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(aann_train.size())\n",
    "    _, aann1_train = aann1(aann_train)\n",
    "    _, aann1_test = aann1(aann_test)\n",
    "    print(aann1_train.size())\n",
    "\n",
    "# loading data\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(aann1_train,aann1_train)\n",
    "dataset_test = torch.utils.data.TensorDataset(aann1_test,aann1_test)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "\n",
    "#aann2 = AANN2()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(aann2.parameters(), lr=learning_rate,lr_decay=0)\n",
    "\n",
    "# training\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(loader):\n",
    "        \n",
    "        inputs, same_inputs = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = aann2(inputs)\n",
    "        loss = criterion(outputs,same_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aannx = AANN()\n",
    "aannx.load_state_dict(torch.load('aann2_task3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing error\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann2(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Train error',end=':')\n",
    "print(error/(num*dim))\n",
    "\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader_test:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann2(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Test error',end=':')\n",
    "print(error/(num*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(aann2.state_dict(),'aann2_task3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AANN3\n",
    "\n",
    "dim = 400 # from previous, AANN2, 500\n",
    "h1_dim = 600\n",
    "h2_dim = 400\n",
    "h3_dim = 300\n",
    "h4_dim = 350\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "num_workers = 8\n",
    "\n",
    "class AANN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AANN3, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.fcl = nn.Linear(h2_dim, h3_dim)\n",
    "        self.fc3 = nn.Linear(h3_dim,h4_dim)\n",
    "        self.fc4 = nn.Linear(h4_dim,dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = nn.Tanh()\n",
    "        x = self.fc1(x)\n",
    "        x = m(self.fc2(x))\n",
    "        y = self.fcl(x)\n",
    "        x = m(self.fc3(y))\n",
    "        x = self.fc4(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for AANN3\n",
    "\n",
    "# loading data\n",
    "with torch.no_grad():\n",
    "    print(aann1_train.size())\n",
    "    _, aann2_train = aann2(aann1_train)\n",
    "    _, aann2_test = aann2(aann1_test)\n",
    "    print(aann2_train.size())\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(aann2_train,aann2_train)\n",
    "dataset_test = torch.utils.data.TensorDataset(aann2_test,aann2_test)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "\n",
    "aann3 = AANN3()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(aann3.parameters(), lr=learning_rate,lr_decay=0)\n",
    "\n",
    "# training\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(loader):\n",
    "        \n",
    "        inputs, same_inputs = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs,_ = aann3(inputs)\n",
    "        loss = criterion(outputs,same_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aannx = AANN3()\n",
    "aannx.load_state_dict(torch.load('aann3_task3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing error\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann3(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Train error',end=':')\n",
    "print(error/(num*dim))\n",
    "\n",
    "error = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for data in loader_test:\n",
    "        inputs, labels = data\n",
    "        outputs,_ = aann3(inputs)\n",
    "        diff = outputs-inputs\n",
    "        error += ((diff*diff).sum().item())\n",
    "        num += 1\n",
    "print('Test error',end=':')\n",
    "print(error/(num*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(aann3.state_dict(),'aann3_task3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aann_train = torch.Tensor(train_data)\n",
    "aann_test = torch.Tensor(test_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(aann_train.size())\n",
    "    _, aann_train = aann1(aann_train)\n",
    "    _, aann_train = aann2(aann_train)\n",
    "    _, aann_train = aann3(aann_train)\n",
    "    _, aann_test = aann1(aann_test)\n",
    "    _, aann_test = aann2(aann_test)\n",
    "    _, aann_test = aann3(aann_test)\n",
    "    print(aann_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "dim = 300 # from output of aann2, 300\n",
    "h1_dim = 50\n",
    "h2_dim = 28\n",
    "out_dim = len(class_names)\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_epochs = 1000\n",
    "batch_size = 32  # general rule is to try 32, 64, 128 ... \n",
    "\n",
    "# DNN model with 2 fully connected hidden layers\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,h1_dim) \n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim) \n",
    "        self.fc3 = nn.Linear(h2_dim,out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fun and update method\n",
    "\n",
    "tensor_train_target = torch.LongTensor(train_target) # long otherwise takes as float\n",
    "\n",
    "tensor_test_target = torch.LongTensor(test_target)\n",
    "\n",
    "# num_workers - how many parallel data loadings\n",
    "train_dataset = torch.utils.data.TensorDataset(aann_train,tensor_train_target)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(aann_test,tensor_test_target)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "net = DNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# ASDG for generalised delta ig\n",
    "# schochastic gradient descent with nestrov momentum\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# optimizer = optim.Adagrad(net.parameters(), lr=learning_rate,lr_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, load_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = load_data\n",
    "        \n",
    "        # initialize with zeros all param\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if((epoch+1)%50 == 0):\n",
    "        print(epoch+1, running_loss/count)\n",
    "        running_loss = 0.0\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DNN()\n",
    "net.load_state_dict(torch.load('net_task3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# train data\n",
    "correct = 0\n",
    "total = 0\n",
    "preds = []\n",
    "tgts = []\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        preds.extend(predicted)\n",
    "        tgts.extend(labels)\n",
    "print('train accuracy',end=':')\n",
    "print(correct/total)\n",
    "\n",
    "ConfusionPlots.pretty_conf(tgts,preds,'Task3-TrainData-3-Stacked-AANN',columns=class_names)\n",
    "\n",
    "#test data\n",
    "correct = 0\n",
    "total = 0\n",
    "preds = []\n",
    "tgts = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        preds.extend(predicted)\n",
    "        tgts.extend(labels)\n",
    "print('test accuracy',end=':')\n",
    "print(correct/total)\n",
    "\n",
    "ConfusionPlots.pretty_conf(tgts,preds,'Task3-TestData-3-Stacked-AANN',columns=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'net_task3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
