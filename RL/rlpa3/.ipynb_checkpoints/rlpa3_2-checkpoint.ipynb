{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import gym\n",
    "import random as rd\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing DQN class\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    REPLAY_MEMORY_SIZE = 5000 \t\t\t# number of tuples in experience replay  \n",
    "    EPSILON = 1 \t\t\t\t\t\t# epsilon of epsilon-greedy exploation\n",
    "    EPSILON_DECAY = 0.999 \t\t\t\t# exponential decay multiplier for epsilon\n",
    "    HIDDEN1_SIZE = 16 \t\t\t\t\t# size of hidden layer 1\n",
    "    HIDDEN2_SIZE = 16 \t\t\t\t\t# size of hidden layer 2\n",
    "    EPISODES_NUM = 2000 \t\t\t\t# number of episodes to train on. Ideally shouldn't take longer than 2000\n",
    "    MAX_STEPS = 200 \t\t\t\t\t# maximum number of steps in an episode \n",
    "    LEARNING_RATE = 0.001 \t\t\t\t# learning rate and other parameters for SGD/RMSProp/Adam\n",
    "    MINIBATCH_SIZE = 10 \t\t\t\t# size of minibatch sampled from the experience replay\n",
    "    DISCOUNT_FACTOR = 0.9 \t\t\t\t# MDP's gamma\n",
    "    TARGET_UPDATE_FREQ = 50 \t\t\t# number of steps (not episodes) after which to update the target networks \n",
    "    LOG_DIR = './logs' \t\t\t\t\t# directory wherein logging takes place\n",
    "    EPSILON_MIN = 0.05\n",
    "\n",
    "\n",
    "    # Create and initialize the environment\n",
    "    def __init__(self, env):\n",
    "        self.env = gym.make(env)\n",
    "        assert len(self.env.observation_space.shape) == 1\n",
    "        self.input_size = self.env.observation_space.shape[0]\t\t# In case of cartpole, 4 state features\n",
    "        self.output_size = self.env.action_space.n\t\t\t\t\t# In case of cartpole, 2 actions (right/left)\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "        self.replay_buffer = None\n",
    "        self.epsilon = self.EPSILON\n",
    "\n",
    "    # Create the Q-network\n",
    "    def initialize_network(self):\n",
    "\n",
    "        ############################################################\n",
    "        # Design your q-network here.\n",
    "        # \n",
    "        # Add hidden layers and the output layer. For instance:\n",
    "        # \n",
    "        # with tf.name_scope('output'):\n",
    "        #\tW_n = tf.Variable(\n",
    "        # \t\t\t tf.truncated_normal([self.HIDDEN_n-1_SIZE, self.output_size], \n",
    "        # \t\t\t stddev=0.01), name='W_n')\n",
    "        # \tb_n = tf.Variable(tf.zeros(self.output_size), name='b_n')\n",
    "        # \tself.Q = tf.matmul(h_n-1, W_n) + b_n\n",
    "        #\n",
    "        #############################################################\n",
    "        \n",
    "        # Model designed using keras layers\n",
    "        self.model = keras.Sequential([\n",
    "                layers.InputLayer(input_shape=(self.input_size,)),\n",
    "                layers.Dense(self.HIDDEN1_SIZE, activation='relu', name='hidden1', kernel_initializer='RandomNormal'),\n",
    "                layers.Dense(self.HIDDEN2_SIZE, activation='relu', name='hidden2', kernel_initializer='RandomNormal'),\n",
    "                layers.Dense(self.output_size, activation='linear', name='output', kernel_initializer='RandomNormal')\n",
    "        ])\n",
    "\n",
    "        ############################################################\n",
    "        # Next, compute the loss.\n",
    "        #\n",
    "        # First, compute the q-values. Note that you need to calculate these\n",
    "        # for the actions in the (s,a,s',r) tuples from the experience replay's minibatch\n",
    "        #\n",
    "        # Next, compute the l2 loss between these estimated q-values and \n",
    "        # the target (which is computed using the frozen target network)\n",
    "        #\n",
    "        ############################################################\n",
    "        \n",
    "        ############################################################\n",
    "        # Finally, choose a gradient descent algorithm : SGD/RMSProp/Adam. \n",
    "        #\n",
    "        # For instance:\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(self.LEARNING_RATE)\n",
    "        # global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
    "        #\n",
    "        ############################################################\n",
    "\n",
    "        # Assigned descent algo. and loss function in one line\n",
    "        self.model.compile(loss=keras.losses.MeanSquaredError(),optimizer=keras.optimizers.Adam(lr=self.LEARNING_RATE))\n",
    "        self.model.summary()\n",
    "        \n",
    "        # create a target model a clone to our model i.e. target network\n",
    "        self.target_model = keras.models.clone_model(self.model)\n",
    "        self.target_model.build((None, self.input_size))\n",
    "        self.target_model.compile(loss=keras.losses.MeanSquaredError(),optimizer=keras.optimizers.Adam(lr=self.LEARNING_RATE))\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        ############################################################\n",
    "\n",
    "    def train(self, episodes_num=EPISODES_NUM):\n",
    "\n",
    "        # Initialize summary for TensorBoard \n",
    "        summary_writer = tf.summary.create_file_writer(self.LOG_DIR)\n",
    "        summary = tf.summary\n",
    "        # Alternatively, you could use animated real-time plots from matplotlib \n",
    "        # (https://stackoverflow.com/a/24228275/3284912)\n",
    "\n",
    "#         # Initialize the TF session\n",
    "#         self.session = tf.Session()\n",
    "#         self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        ############################################################\n",
    "        # Initialize other variables (like the replay memory)\n",
    "        ############################################################\n",
    "        \n",
    "        # Using deque\n",
    "        self.replay_buffer = deque(maxlen=self.REPLAY_MEMORY_SIZE)\n",
    "        total_steps = 0\n",
    "\n",
    "        ############################################################\n",
    "        # Main training loop\n",
    "        # \n",
    "        # In each episode, \n",
    "        #\tpick the action for the given state, \n",
    "        #\tperform a 'step' in the environment to get the reward and next state,\n",
    "        #\tupdate the replay buffer,\n",
    "        #\tsample a random minibatch from the replay buffer,\n",
    "        # \tperform Q-learning,\n",
    "        #\tupdate the target network, if required.\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        # You'll need to write code in various places in the following skeleton\n",
    "        #\n",
    "        ############################################################\n",
    "\n",
    "        for episode in range(episodes_num):\n",
    "\n",
    "            state = np.array([self.env.reset()])\n",
    "\n",
    "            ############################################################\n",
    "            # Episode-specific initializations go here.\n",
    "            ############################################################\n",
    "            \n",
    "            episode_length = 0\n",
    "            score = 0\n",
    "            \n",
    "            ############################################################\n",
    "\n",
    "            while True:\n",
    "                ############################################################\n",
    "                # Pick the next action using epsilon greedy and execute it\n",
    "                ############################################################\n",
    "                \n",
    "                episode_length += 1\n",
    "                total_steps += 1\n",
    "                if(rd.random() < self.epsilon):\n",
    "                    act = self.env.action_space.sample()\n",
    "                else:\n",
    "                    act = np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "                ############################################################\n",
    "                # Step in the environment. Something like: \n",
    "                # next_state, reward, done, _ = self.env.step(action)\n",
    "                ############################################################\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(act)\n",
    "                next_state = np.array([next_state])\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                \n",
    "                ############################################################\n",
    "                # Update the (limited) replay buffer. \n",
    "                #\n",
    "                # Note : when the replay buffer is full, you'll need to \n",
    "                # remove an entry to accommodate a new one.\n",
    "                ############################################################\n",
    "\n",
    "                # The max length in deque removes oldest if buffer size exceeds it\n",
    "                self.replay_buffer.append((state,act,reward,next_state,done))\n",
    "\n",
    "                ############################################################\n",
    "                # Sample a random minibatch and perform Q-learning (fetch max Q at s') \n",
    "                #\n",
    "                # Remember, the target (r + gamma * max Q) is computed    \n",
    "                # with the help of the target network.\n",
    "                # Compute this target and pass it to the network for computing \n",
    "                # and minimizing the loss with the current estimates\n",
    "                #\n",
    "                ############################################################\n",
    "                \n",
    "                # not starting network update until it has a batch size elements\n",
    "                if len(self.replay_buffer) == self.REPLAY_MEMORY_SIZE:\n",
    "                    replay_batch = rd.sample(self.replay_buffer,self.MINIBATCH_SIZE)\n",
    "                    for st, act, rwd, nst, d in replay_batch:\n",
    "                        if d:\n",
    "                            y = rwd\n",
    "                        else:\n",
    "                            y = (rwd + self.DISCOUNT_FACTOR * np.max(self.target_model.predict(nst)[0]))\n",
    "                        target = self.model.predict(state)\n",
    "                        target[0][act] = y\n",
    "                        self.model.fit(st, target, epochs = 1, verbose = 0) # verbose = 0\n",
    "                        \n",
    "                    if self.epsilon > self.EPSILON_MIN:\n",
    "                        self.epsilon *= self.EPSILON_DECAY\n",
    "\n",
    "                ############################################################\n",
    "                # Update target weights. \n",
    "                #\n",
    "                # Something along the lines of:\n",
    "                # if total_steps % self.TARGET_UPDATE_FREQ == 0:\n",
    "                # \ttarget_weights = self.session.run(self.weights)\n",
    "                ############################################################\n",
    "\n",
    "                if total_steps%self.TARGET_UPDATE_FREQ == 0:\n",
    "                    self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "                ############################################################\n",
    "                # Break out of the loop if the episode ends\n",
    "                #\n",
    "                # Something like:\n",
    "                # if done or (episode_length == self.MAX_STEPS):\n",
    "                # \tbreak\n",
    "                #\n",
    "                ############################################################\n",
    "                \n",
    "                if done or episode_length == self.MAX_STEPS:\n",
    "                    break\n",
    "\n",
    "\n",
    "            ############################################################\n",
    "            # Logging. \n",
    "            #\n",
    "            # Very important. This is what gives an idea of how good the current\n",
    "            # experiment is, and if one should terminate and re-run with new parameters\n",
    "            # The earlier you learn how to read and visualize experiment logs quickly,\n",
    "            # the faster you'll be able to prototype and learn.\n",
    "            #\n",
    "            # Use any debugging information you think you need.\n",
    "            # For instance :\n",
    "\n",
    "            print(\"Training: Episode = %d, Length = %d, Global step = %d\" % (episode, episode_length, total_steps))\n",
    "            with summary_writer.as_default():\n",
    "                summary.scalar(\"episode length\",episode ,step=episode_length)\n",
    "\n",
    "\n",
    "    # Simple function to visually 'test' a policy\n",
    "    def playPolicy(self):\n",
    "\n",
    "        done = False\n",
    "        steps = 0\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # we assume the CartPole task to be solved if the pole remains upright for 200 steps\n",
    "        while not done and steps < 200: \n",
    "            self.env.render()\n",
    "            action = np.argmax(self.target_model.predict(state)[0])\n",
    "            state, _, done, _ = self.env.step(action)\n",
    "            steps += 1\n",
    "\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Training: Episode = 0, Length = 37, Global step = 37\n",
      "Training: Episode = 1, Length = 15, Global step = 52\n",
      "Training: Episode = 2, Length = 28, Global step = 80\n",
      "Training: Episode = 3, Length = 13, Global step = 93\n",
      "Training: Episode = 4, Length = 13, Global step = 106\n",
      "Training: Episode = 5, Length = 19, Global step = 125\n",
      "Training: Episode = 6, Length = 17, Global step = 142\n",
      "Training: Episode = 7, Length = 8, Global step = 150\n",
      "Training: Episode = 8, Length = 24, Global step = 174\n",
      "Training: Episode = 9, Length = 13, Global step = 187\n",
      "Training: Episode = 10, Length = 13, Global step = 200\n",
      "Training: Episode = 11, Length = 16, Global step = 216\n",
      "Training: Episode = 12, Length = 14, Global step = 230\n",
      "Training: Episode = 13, Length = 31, Global step = 261\n",
      "Training: Episode = 14, Length = 33, Global step = 294\n",
      "Training: Episode = 15, Length = 30, Global step = 324\n",
      "Training: Episode = 16, Length = 15, Global step = 339\n",
      "Training: Episode = 17, Length = 74, Global step = 413\n",
      "Training: Episode = 18, Length = 23, Global step = 436\n",
      "Training: Episode = 19, Length = 12, Global step = 448\n",
      "Training: Episode = 20, Length = 17, Global step = 465\n",
      "Training: Episode = 21, Length = 17, Global step = 482\n",
      "Training: Episode = 22, Length = 23, Global step = 505\n",
      "Training: Episode = 23, Length = 20, Global step = 525\n",
      "Training: Episode = 24, Length = 22, Global step = 547\n",
      "Training: Episode = 25, Length = 22, Global step = 569\n",
      "Training: Episode = 26, Length = 14, Global step = 583\n",
      "Training: Episode = 27, Length = 14, Global step = 597\n",
      "Training: Episode = 28, Length = 31, Global step = 628\n",
      "Training: Episode = 29, Length = 9, Global step = 637\n",
      "Training: Episode = 30, Length = 52, Global step = 689\n",
      "Training: Episode = 31, Length = 21, Global step = 710\n",
      "Training: Episode = 32, Length = 12, Global step = 722\n",
      "Training: Episode = 33, Length = 18, Global step = 740\n",
      "Training: Episode = 34, Length = 17, Global step = 757\n",
      "Training: Episode = 35, Length = 15, Global step = 772\n",
      "Training: Episode = 36, Length = 32, Global step = 804\n",
      "Training: Episode = 37, Length = 16, Global step = 820\n",
      "Training: Episode = 38, Length = 50, Global step = 870\n",
      "Training: Episode = 39, Length = 43, Global step = 913\n",
      "Training: Episode = 40, Length = 40, Global step = 953\n",
      "Training: Episode = 41, Length = 21, Global step = 974\n",
      "Training: Episode = 42, Length = 13, Global step = 987\n",
      "Training: Episode = 43, Length = 45, Global step = 1032\n",
      "Training: Episode = 44, Length = 16, Global step = 1048\n",
      "Training: Episode = 45, Length = 21, Global step = 1069\n",
      "Training: Episode = 46, Length = 23, Global step = 1092\n",
      "Training: Episode = 47, Length = 19, Global step = 1111\n",
      "Training: Episode = 48, Length = 51, Global step = 1162\n",
      "Training: Episode = 49, Length = 14, Global step = 1176\n",
      "Training: Episode = 50, Length = 14, Global step = 1190\n",
      "Training: Episode = 51, Length = 9, Global step = 1199\n",
      "Training: Episode = 52, Length = 13, Global step = 1212\n",
      "Training: Episode = 53, Length = 13, Global step = 1225\n",
      "Training: Episode = 54, Length = 13, Global step = 1238\n",
      "Training: Episode = 55, Length = 32, Global step = 1270\n",
      "Training: Episode = 56, Length = 13, Global step = 1283\n",
      "Training: Episode = 57, Length = 32, Global step = 1315\n",
      "Training: Episode = 58, Length = 28, Global step = 1343\n",
      "Training: Episode = 59, Length = 14, Global step = 1357\n",
      "Training: Episode = 60, Length = 17, Global step = 1374\n",
      "Training: Episode = 61, Length = 16, Global step = 1390\n",
      "Training: Episode = 62, Length = 43, Global step = 1433\n",
      "Training: Episode = 63, Length = 57, Global step = 1490\n",
      "Training: Episode = 64, Length = 23, Global step = 1513\n",
      "Training: Episode = 65, Length = 29, Global step = 1542\n",
      "Training: Episode = 66, Length = 22, Global step = 1564\n",
      "Training: Episode = 67, Length = 16, Global step = 1580\n",
      "Training: Episode = 68, Length = 18, Global step = 1598\n",
      "Training: Episode = 69, Length = 13, Global step = 1611\n",
      "Training: Episode = 70, Length = 14, Global step = 1625\n",
      "Training: Episode = 71, Length = 33, Global step = 1658\n",
      "Training: Episode = 72, Length = 51, Global step = 1709\n",
      "Training: Episode = 73, Length = 25, Global step = 1734\n",
      "Training: Episode = 74, Length = 23, Global step = 1757\n",
      "Training: Episode = 75, Length = 12, Global step = 1769\n",
      "Training: Episode = 76, Length = 22, Global step = 1791\n",
      "Training: Episode = 77, Length = 17, Global step = 1808\n",
      "Training: Episode = 78, Length = 14, Global step = 1822\n",
      "Training: Episode = 79, Length = 17, Global step = 1839\n",
      "Training: Episode = 80, Length = 21, Global step = 1860\n",
      "Training: Episode = 81, Length = 16, Global step = 1876\n",
      "Training: Episode = 82, Length = 15, Global step = 1891\n",
      "Training: Episode = 83, Length = 17, Global step = 1908\n",
      "Training: Episode = 84, Length = 11, Global step = 1919\n",
      "Training: Episode = 85, Length = 16, Global step = 1935\n",
      "Training: Episode = 86, Length = 16, Global step = 1951\n",
      "Training: Episode = 87, Length = 40, Global step = 1991\n",
      "Training: Episode = 88, Length = 16, Global step = 2007\n",
      "Training: Episode = 89, Length = 20, Global step = 2027\n",
      "Training: Episode = 90, Length = 27, Global step = 2054\n",
      "Training: Episode = 91, Length = 14, Global step = 2068\n",
      "Training: Episode = 92, Length = 17, Global step = 2085\n",
      "Training: Episode = 93, Length = 41, Global step = 2126\n",
      "Training: Episode = 94, Length = 17, Global step = 2143\n",
      "Training: Episode = 95, Length = 11, Global step = 2154\n",
      "Training: Episode = 96, Length = 51, Global step = 2205\n",
      "Training: Episode = 97, Length = 12, Global step = 2217\n",
      "Training: Episode = 98, Length = 14, Global step = 2231\n",
      "Training: Episode = 99, Length = 13, Global step = 2244\n",
      "Training: Episode = 100, Length = 14, Global step = 2258\n",
      "Training: Episode = 101, Length = 13, Global step = 2271\n",
      "Training: Episode = 102, Length = 13, Global step = 2284\n",
      "Training: Episode = 103, Length = 23, Global step = 2307\n",
      "Training: Episode = 104, Length = 14, Global step = 2321\n",
      "Training: Episode = 105, Length = 11, Global step = 2332\n",
      "Training: Episode = 106, Length = 15, Global step = 2347\n",
      "Training: Episode = 107, Length = 28, Global step = 2375\n",
      "Training: Episode = 108, Length = 18, Global step = 2393\n",
      "Training: Episode = 109, Length = 13, Global step = 2406\n",
      "Training: Episode = 110, Length = 34, Global step = 2440\n",
      "Training: Episode = 111, Length = 35, Global step = 2475\n",
      "Training: Episode = 112, Length = 10, Global step = 2485\n",
      "Training: Episode = 113, Length = 19, Global step = 2504\n",
      "Training: Episode = 114, Length = 14, Global step = 2518\n",
      "Training: Episode = 115, Length = 22, Global step = 2540\n",
      "Training: Episode = 116, Length = 32, Global step = 2572\n",
      "Training: Episode = 117, Length = 28, Global step = 2600\n",
      "Training: Episode = 118, Length = 35, Global step = 2635\n",
      "Training: Episode = 119, Length = 17, Global step = 2652\n",
      "Training: Episode = 120, Length = 33, Global step = 2685\n",
      "Training: Episode = 121, Length = 16, Global step = 2701\n",
      "Training: Episode = 122, Length = 19, Global step = 2720\n",
      "Training: Episode = 123, Length = 26, Global step = 2746\n",
      "Training: Episode = 124, Length = 27, Global step = 2773\n",
      "Training: Episode = 125, Length = 14, Global step = 2787\n",
      "Training: Episode = 126, Length = 25, Global step = 2812\n",
      "Training: Episode = 127, Length = 28, Global step = 2840\n",
      "Training: Episode = 128, Length = 14, Global step = 2854\n",
      "Training: Episode = 129, Length = 11, Global step = 2865\n",
      "Training: Episode = 130, Length = 21, Global step = 2886\n",
      "Training: Episode = 131, Length = 32, Global step = 2918\n",
      "Training: Episode = 132, Length = 21, Global step = 2939\n",
      "Training: Episode = 133, Length = 22, Global step = 2961\n",
      "Training: Episode = 134, Length = 10, Global step = 2971\n",
      "Training: Episode = 135, Length = 30, Global step = 3001\n",
      "Training: Episode = 136, Length = 12, Global step = 3013\n",
      "Training: Episode = 137, Length = 21, Global step = 3034\n",
      "Training: Episode = 138, Length = 16, Global step = 3050\n",
      "Training: Episode = 139, Length = 23, Global step = 3073\n",
      "Training: Episode = 140, Length = 10, Global step = 3083\n",
      "Training: Episode = 141, Length = 20, Global step = 3103\n",
      "Training: Episode = 142, Length = 41, Global step = 3144\n",
      "Training: Episode = 143, Length = 24, Global step = 3168\n",
      "Training: Episode = 144, Length = 10, Global step = 3178\n",
      "Training: Episode = 145, Length = 14, Global step = 3192\n",
      "Training: Episode = 146, Length = 12, Global step = 3204\n",
      "Training: Episode = 147, Length = 13, Global step = 3217\n",
      "Training: Episode = 148, Length = 16, Global step = 3233\n",
      "Training: Episode = 149, Length = 15, Global step = 3248\n",
      "Training: Episode = 150, Length = 35, Global step = 3283\n",
      "Training: Episode = 151, Length = 26, Global step = 3309\n",
      "Training: Episode = 152, Length = 17, Global step = 3326\n",
      "Training: Episode = 153, Length = 18, Global step = 3344\n",
      "Training: Episode = 154, Length = 13, Global step = 3357\n",
      "Training: Episode = 155, Length = 19, Global step = 3376\n",
      "Training: Episode = 156, Length = 16, Global step = 3392\n",
      "Training: Episode = 157, Length = 28, Global step = 3420\n",
      "Training: Episode = 158, Length = 11, Global step = 3431\n",
      "Training: Episode = 159, Length = 35, Global step = 3466\n",
      "Training: Episode = 160, Length = 15, Global step = 3481\n",
      "Training: Episode = 161, Length = 18, Global step = 3499\n",
      "Training: Episode = 162, Length = 21, Global step = 3520\n",
      "Training: Episode = 163, Length = 40, Global step = 3560\n",
      "Training: Episode = 164, Length = 19, Global step = 3579\n",
      "Training: Episode = 165, Length = 14, Global step = 3593\n",
      "Training: Episode = 166, Length = 19, Global step = 3612\n",
      "Training: Episode = 167, Length = 26, Global step = 3638\n",
      "Training: Episode = 168, Length = 21, Global step = 3659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Episode = 169, Length = 21, Global step = 3680\n",
      "Training: Episode = 170, Length = 20, Global step = 3700\n",
      "Training: Episode = 171, Length = 20, Global step = 3720\n",
      "Training: Episode = 172, Length = 13, Global step = 3733\n",
      "Training: Episode = 173, Length = 14, Global step = 3747\n",
      "Training: Episode = 174, Length = 15, Global step = 3762\n",
      "Training: Episode = 175, Length = 28, Global step = 3790\n",
      "Training: Episode = 176, Length = 24, Global step = 3814\n",
      "Training: Episode = 177, Length = 19, Global step = 3833\n",
      "Training: Episode = 178, Length = 57, Global step = 3890\n",
      "Training: Episode = 179, Length = 10, Global step = 3900\n",
      "Training: Episode = 180, Length = 15, Global step = 3915\n",
      "Training: Episode = 181, Length = 39, Global step = 3954\n",
      "Training: Episode = 182, Length = 13, Global step = 3967\n",
      "Training: Episode = 183, Length = 16, Global step = 3983\n",
      "Training: Episode = 184, Length = 14, Global step = 3997\n",
      "Training: Episode = 185, Length = 17, Global step = 4014\n",
      "Training: Episode = 186, Length = 26, Global step = 4040\n",
      "Training: Episode = 187, Length = 19, Global step = 4059\n",
      "Training: Episode = 188, Length = 17, Global step = 4076\n",
      "Training: Episode = 189, Length = 18, Global step = 4094\n",
      "Training: Episode = 190, Length = 23, Global step = 4117\n",
      "Training: Episode = 191, Length = 25, Global step = 4142\n",
      "Training: Episode = 192, Length = 27, Global step = 4169\n",
      "Training: Episode = 193, Length = 11, Global step = 4180\n",
      "Training: Episode = 194, Length = 61, Global step = 4241\n",
      "Training: Episode = 195, Length = 15, Global step = 4256\n",
      "Training: Episode = 196, Length = 11, Global step = 4267\n",
      "Training: Episode = 197, Length = 34, Global step = 4301\n",
      "Training: Episode = 198, Length = 10, Global step = 4311\n",
      "Training: Episode = 199, Length = 32, Global step = 4343\n",
      "Training: Episode = 200, Length = 21, Global step = 4364\n",
      "Training: Episode = 201, Length = 43, Global step = 4407\n",
      "Training: Episode = 202, Length = 12, Global step = 4419\n",
      "Training: Episode = 203, Length = 24, Global step = 4443\n",
      "Training: Episode = 204, Length = 17, Global step = 4460\n",
      "Training: Episode = 205, Length = 32, Global step = 4492\n",
      "Training: Episode = 206, Length = 11, Global step = 4503\n",
      "Training: Episode = 207, Length = 10, Global step = 4513\n",
      "Training: Episode = 208, Length = 27, Global step = 4540\n",
      "Training: Episode = 209, Length = 20, Global step = 4560\n",
      "Training: Episode = 210, Length = 26, Global step = 4586\n",
      "Training: Episode = 211, Length = 14, Global step = 4600\n",
      "Training: Episode = 212, Length = 13, Global step = 4613\n",
      "Training: Episode = 213, Length = 22, Global step = 4635\n",
      "Training: Episode = 214, Length = 19, Global step = 4654\n",
      "Training: Episode = 215, Length = 36, Global step = 4690\n",
      "Training: Episode = 216, Length = 17, Global step = 4707\n",
      "Training: Episode = 217, Length = 12, Global step = 4719\n",
      "Training: Episode = 218, Length = 17, Global step = 4736\n",
      "Training: Episode = 219, Length = 17, Global step = 4753\n",
      "Training: Episode = 220, Length = 25, Global step = 4778\n",
      "Training: Episode = 221, Length = 14, Global step = 4792\n",
      "Training: Episode = 222, Length = 17, Global step = 4809\n",
      "Training: Episode = 223, Length = 23, Global step = 4832\n",
      "Training: Episode = 224, Length = 56, Global step = 4888\n",
      "Training: Episode = 225, Length = 38, Global step = 4926\n",
      "Training: Episode = 226, Length = 44, Global step = 4970\n",
      "Training: Episode = 227, Length = 16, Global step = 4986\n",
      "Training: Episode = 228, Length = 18, Global step = 5004\n",
      "Training: Episode = 229, Length = 32, Global step = 5036\n",
      "Training: Episode = 230, Length = 27, Global step = 5063\n",
      "Training: Episode = 231, Length = 40, Global step = 5103\n",
      "Training: Episode = 232, Length = 17, Global step = 5120\n",
      "Training: Episode = 233, Length = 13, Global step = 5133\n",
      "Training: Episode = 234, Length = 42, Global step = 5175\n",
      "Training: Episode = 235, Length = 19, Global step = 5194\n",
      "Training: Episode = 236, Length = 16, Global step = 5210\n",
      "Training: Episode = 237, Length = 20, Global step = 5230\n",
      "Training: Episode = 238, Length = 26, Global step = 5256\n",
      "Training: Episode = 239, Length = 11, Global step = 5267\n",
      "Training: Episode = 240, Length = 12, Global step = 5279\n",
      "Training: Episode = 241, Length = 10, Global step = 5289\n",
      "Training: Episode = 242, Length = 21, Global step = 5310\n",
      "Training: Episode = 243, Length = 33, Global step = 5343\n",
      "Training: Episode = 244, Length = 28, Global step = 5371\n",
      "Training: Episode = 245, Length = 16, Global step = 5387\n",
      "Training: Episode = 246, Length = 38, Global step = 5425\n",
      "Training: Episode = 247, Length = 16, Global step = 5441\n",
      "Training: Episode = 248, Length = 10, Global step = 5451\n",
      "Training: Episode = 249, Length = 10, Global step = 5461\n",
      "Training: Episode = 250, Length = 9, Global step = 5470\n",
      "Training: Episode = 251, Length = 19, Global step = 5489\n",
      "Training: Episode = 252, Length = 25, Global step = 5514\n",
      "Training: Episode = 253, Length = 14, Global step = 5528\n",
      "Training: Episode = 254, Length = 11, Global step = 5539\n",
      "Training: Episode = 255, Length = 13, Global step = 5552\n",
      "Training: Episode = 256, Length = 15, Global step = 5567\n",
      "Training: Episode = 257, Length = 10, Global step = 5577\n",
      "Training: Episode = 258, Length = 12, Global step = 5589\n",
      "Training: Episode = 259, Length = 14, Global step = 5603\n",
      "Training: Episode = 260, Length = 9, Global step = 5612\n",
      "Training: Episode = 261, Length = 27, Global step = 5639\n",
      "Training: Episode = 262, Length = 36, Global step = 5675\n",
      "Training: Episode = 263, Length = 14, Global step = 5689\n",
      "Training: Episode = 264, Length = 10, Global step = 5699\n",
      "Training: Episode = 265, Length = 18, Global step = 5717\n",
      "Training: Episode = 266, Length = 12, Global step = 5729\n",
      "Training: Episode = 267, Length = 18, Global step = 5747\n",
      "Training: Episode = 268, Length = 13, Global step = 5760\n",
      "Training: Episode = 269, Length = 13, Global step = 5773\n",
      "Training: Episode = 270, Length = 13, Global step = 5786\n",
      "Training: Episode = 271, Length = 18, Global step = 5804\n",
      "Training: Episode = 272, Length = 9, Global step = 5813\n",
      "Training: Episode = 273, Length = 19, Global step = 5832\n",
      "Training: Episode = 274, Length = 8, Global step = 5840\n",
      "Training: Episode = 275, Length = 12, Global step = 5852\n",
      "Training: Episode = 276, Length = 21, Global step = 5873\n",
      "Training: Episode = 277, Length = 15, Global step = 5888\n",
      "Training: Episode = 278, Length = 12, Global step = 5900\n",
      "Training: Episode = 279, Length = 12, Global step = 5912\n",
      "Training: Episode = 280, Length = 9, Global step = 5921\n",
      "Training: Episode = 281, Length = 11, Global step = 5932\n",
      "Training: Episode = 282, Length = 10, Global step = 5942\n",
      "Training: Episode = 283, Length = 14, Global step = 5956\n",
      "Training: Episode = 284, Length = 10, Global step = 5966\n",
      "Training: Episode = 285, Length = 13, Global step = 5979\n",
      "Training: Episode = 286, Length = 12, Global step = 5991\n",
      "Training: Episode = 287, Length = 11, Global step = 6002\n",
      "Training: Episode = 288, Length = 10, Global step = 6012\n",
      "Training: Episode = 289, Length = 11, Global step = 6023\n",
      "Training: Episode = 290, Length = 12, Global step = 6035\n",
      "Training: Episode = 291, Length = 10, Global step = 6045\n",
      "Training: Episode = 292, Length = 10, Global step = 6055\n",
      "Training: Episode = 293, Length = 10, Global step = 6065\n",
      "Training: Episode = 294, Length = 17, Global step = 6082\n",
      "Training: Episode = 295, Length = 29, Global step = 6111\n",
      "Training: Episode = 296, Length = 11, Global step = 6122\n",
      "Training: Episode = 297, Length = 14, Global step = 6136\n",
      "Training: Episode = 298, Length = 10, Global step = 6146\n",
      "Training: Episode = 299, Length = 10, Global step = 6156\n",
      "Training: Episode = 300, Length = 12, Global step = 6168\n",
      "Training: Episode = 301, Length = 12, Global step = 6180\n",
      "Training: Episode = 302, Length = 12, Global step = 6192\n",
      "Training: Episode = 303, Length = 11, Global step = 6203\n",
      "Training: Episode = 304, Length = 9, Global step = 6212\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize the model\n",
    "dqn = DQN('CartPole-v0')\n",
    "dqn.initialize_network()\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "dqn.train()\n",
    "print(\"\\nFinished training...\\nCheck out some demonstrations\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned behaviour for a few episodes\n",
    "results = []\n",
    "for i in range(50):\n",
    "    episode_length = dqn.playPolicy()\n",
    "    print(\"Test steps = \", episode_length)\n",
    "    results.append(episode_length)\n",
    "print(\"Mean steps = \", sum(results) / len(results))\t\n",
    "\n",
    "print(\"\\nFinished.\")\n",
    "print(\"\\nCiao, and hasta la vista...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
