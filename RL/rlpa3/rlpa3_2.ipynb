{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import gym\n",
    "import random as rd\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing DQN class\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    REPLAY_MEMORY_SIZE = 5000 \t\t\t# number of tuples in experience replay  \n",
    "    EPSILON = 1 \t\t\t\t\t\t# epsilon of epsilon-greedy exploation\n",
    "    EPSILON_DECAY = 0.999 \t\t\t\t# exponential decay multiplier for epsilon\n",
    "    HIDDEN1_SIZE = 16 \t\t\t\t\t# size of hidden layer 1\n",
    "    HIDDEN2_SIZE = 16 \t\t\t\t\t# size of hidden layer 2\n",
    "    EPISODES_NUM = 2000 \t\t\t\t# number of episodes to train on. Ideally shouldn't take longer than 2000\n",
    "    MAX_STEPS = 200 \t\t\t\t\t# maximum number of steps in an episode \n",
    "    LEARNING_RATE = 0.001 \t\t\t\t# learning rate and other parameters for SGD/RMSProp/Adam\n",
    "    MINIBATCH_SIZE = 10 \t\t\t\t# size of minibatch sampled from the experience replay\n",
    "    DISCOUNT_FACTOR = 0.9 \t\t\t\t# MDP's gamma\n",
    "    TARGET_UPDATE_FREQ = 50 \t\t\t# number of steps (not episodes) after which to update the target networks \n",
    "    LOG_DIR = './logs' \t\t\t\t\t# directory wherein logging takes place\n",
    "    EPSILON_MIN = 0.05\n",
    "\n",
    "\n",
    "    # Create and initialize the environment\n",
    "    def __init__(self, env):\n",
    "        self.env = gym.make(env)\n",
    "        assert len(self.env.observation_space.shape) == 1\n",
    "        self.input_size = self.env.observation_space.shape[0]\t\t# In case of cartpole, 4 state features\n",
    "        self.output_size = self.env.action_space.n\t\t\t\t\t# In case of cartpole, 2 actions (right/left)\n",
    "        self.epsilon = self.EPSILON\n",
    "\n",
    "    # Create the Q-network\n",
    "    def initialize_network(self):\n",
    "\n",
    "        ############################################################\n",
    "        # Design your q-network here.\n",
    "        # \n",
    "        # Add hidden layers and the output layer. For instance:\n",
    "        # \n",
    "        # with tf.name_scope('output'):\n",
    "        #\tW_n = tf.Variable(\n",
    "        # \t\t\t tf.truncated_normal([self.HIDDEN_n-1_SIZE, self.output_size], \n",
    "        # \t\t\t stddev=0.01), name='W_n')\n",
    "        # \tb_n = tf.Variable(tf.zeros(self.output_size), name='b_n')\n",
    "        # \tself.Q = tf.matmul(h_n-1, W_n) + b_n\n",
    "        #\n",
    "        #############################################################\n",
    "        \n",
    "        # Model designed using keras layers\n",
    "        self.model = keras.Sequential([\n",
    "                layers.InputLayer(input_shape=(self.input_size,)),\n",
    "                layers.Dense(self.HIDDEN1_SIZE, activation='relu', name='hidden1', kernel_initializer='RandomNormal'),\n",
    "                layers.Dense(self.HIDDEN2_SIZE, activation='relu', name='hidden2', kernel_initializer='RandomNormal'),\n",
    "                layers.Dense(self.output_size, activation='linear', name='output', kernel_initializer='RandomNormal')\n",
    "        ])\n",
    "\n",
    "        ############################################################\n",
    "        # Next, compute the loss.\n",
    "        #\n",
    "        # First, compute the q-values. Note that you need to calculate these\n",
    "        # for the actions in the (s,a,s',r) tuples from the experience replay's minibatch\n",
    "        #\n",
    "        # Next, compute the l2 loss between these estimated q-values and \n",
    "        # the target (which is computed using the frozen target network)\n",
    "        #\n",
    "        ############################################################\n",
    "        \n",
    "        ############################################################\n",
    "        # Finally, choose a gradient descent algorithm : SGD/RMSProp/Adam. \n",
    "        #\n",
    "        # For instance:\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(self.LEARNING_RATE)\n",
    "        # global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
    "        #\n",
    "        ############################################################\n",
    "\n",
    "        # Assigned descent algo. and loss function in one line\n",
    "        self.model.compile(loss=keras.losses.MeanSquaredError(),optimizer=keras.optimizers.Adam(lr=self.LEARNING_RATE))\n",
    "        self.model.summary()\n",
    "        \n",
    "        # create a target model a clone to our model i.e. target network\n",
    "        self.target_model = keras.models.clone_model(self.model)\n",
    "        self.target_model.build((None, self.input_size))\n",
    "        self.target_model.compile(loss=keras.losses.MeanSquaredError(),optimizer=keras.optimizers.Adam(lr=self.LEARNING_RATE))\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        ############################################################\n",
    "\n",
    "    def train(self, episodes_num=EPISODES_NUM):\n",
    "\n",
    "        # Initialize summary for TensorBoard \n",
    "        summary_writer = tf.summary.create_file_writer(self.LOG_DIR)\n",
    "        summary = tf.summary\n",
    "        # Alternatively, you could use animated real-time plots from matplotlib \n",
    "        # (https://stackoverflow.com/a/24228275/3284912)\n",
    "\n",
    "#         # Initialize the TF session\n",
    "#         self.session = tf.Session()\n",
    "#         self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        ############################################################\n",
    "        # Initialize other variables (like the replay memory)\n",
    "        ############################################################\n",
    "        \n",
    "        # Using deque\n",
    "        self.replay_buffer = deque(maxlen=self.REPLAY_MEMORY_SIZE)\n",
    "        total_steps = 0\n",
    "\n",
    "        ############################################################\n",
    "        # Main training loop\n",
    "        # \n",
    "        # In each episode, \n",
    "        #\tpick the action for the given state, \n",
    "        #\tperform a 'step' in the environment to get the reward and next state,\n",
    "        #\tupdate the replay buffer,\n",
    "        #\tsample a random minibatch from the replay buffer,\n",
    "        # \tperform Q-learning,\n",
    "        #\tupdate the target network, if required.\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        # You'll need to write code in various places in the following skeleton\n",
    "        #\n",
    "        ############################################################\n",
    "\n",
    "        for episode in range(episodes_num):\n",
    "\n",
    "            state = np.array([self.env.reset()])\n",
    "\n",
    "            ############################################################\n",
    "            # Episode-specific initializations go here.\n",
    "            ############################################################\n",
    "            \n",
    "            episode_length = 0\n",
    "            score = 0\n",
    "            \n",
    "            ############################################################\n",
    "\n",
    "            while True:\n",
    "                ############################################################\n",
    "                # Pick the next action using epsilon greedy and execute it\n",
    "                ############################################################\n",
    "                \n",
    "                episode_length += 1\n",
    "                total_steps += 1\n",
    "                if(rd.random() < self.epsilon):\n",
    "                    act = self.env.action_space.sample()\n",
    "                else:\n",
    "                    act = np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "                ############################################################\n",
    "                # Step in the environment. Something like: \n",
    "                # next_state, reward, done, _ = self.env.step(action)\n",
    "                ############################################################\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(act)\n",
    "                next_state = np.array([next_state])\n",
    "                score += reward\n",
    "                \n",
    "                ############################################################\n",
    "                # Update the (limited) replay buffer. \n",
    "                #\n",
    "                # Note : when the replay buffer is full, you'll need to \n",
    "                # remove an entry to accommodate a new one.\n",
    "                ############################################################\n",
    "\n",
    "                # The max length in deque removes oldest if buffer size exceeds it\n",
    "                self.replay_buffer.append((state,act,reward,next_state,done))\n",
    "                state = next_state\n",
    "\n",
    "                ############################################################\n",
    "                # Sample a random minibatch and perform Q-learning (fetch max Q at s') \n",
    "                #\n",
    "                # Remember, the target (r + gamma * max Q) is computed    \n",
    "                # with the help of the target network.\n",
    "                # Compute this target and pass it to the network for computing \n",
    "                # and minimizing the loss with the current estimates\n",
    "                #\n",
    "                ############################################################\n",
    "                \n",
    "                # not starting network update until it has a batch size elements\n",
    "                if len(self.replay_buffer) == self.REPLAY_MEMORY_SIZE:\n",
    "                    batch_states = np.zeros((self.MINIBATCH_SIZE,))\n",
    "                    batch_targets = np.zeros((self.MINIBATCH_SIZE,))\n",
    "                    replay_batch = rd.sample(self.replay_buffer,self.MINIBATCH_SIZE)\n",
    "                    b = 0\n",
    "                    for st, act, rwd, nst, d in replay_batch:\n",
    "                        if d:\n",
    "                            y = rwd\n",
    "                        else:\n",
    "                            y = (rwd + self.DISCOUNT_FACTOR * np.max(self.target_model.predict(nst)[0]))\n",
    "                        tgt = self.model.predict(state)\n",
    "                        tgt[0][act] = y\n",
    "                        batch_states.append(st)\n",
    "                        batch_targets.append(tgt)\n",
    "                        \n",
    "                    self.model.fit(batch_states, batch_targets, epochs=1, verbose = 0)\n",
    "                    \n",
    "                    if self.epsilon > self.EPSILON_MIN:\n",
    "                        self.epsilon *= self.EPSILON_DECAY\n",
    "\n",
    "                ############################################################\n",
    "                # Update target weights. \n",
    "                #\n",
    "                # Something along the lines of:\n",
    "                # if total_steps % self.TARGET_UPDATE_FREQ == 0:\n",
    "                # \ttarget_weights = self.session.run(self.weights)\n",
    "                ############################################################\n",
    "\n",
    "                if total_steps%self.TARGET_UPDATE_FREQ == 0:\n",
    "                    self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "                ############################################################\n",
    "                # Break out of the loop if the episode ends\n",
    "                #\n",
    "                # Something like:\n",
    "                # if done or (episode_length == self.MAX_STEPS):\n",
    "                # \tbreak\n",
    "                #\n",
    "                ############################################################\n",
    "                \n",
    "                if done or episode_length == self.MAX_STEPS:\n",
    "                    break\n",
    "\n",
    "\n",
    "            ############################################################\n",
    "            # Logging. \n",
    "            #\n",
    "            # Very important. This is what gives an idea of how good the current\n",
    "            # experiment is, and if one should terminate and re-run with new parameters\n",
    "            # The earlier you learn how to read and visualize experiment logs quickly,\n",
    "            # the faster you'll be able to prototype and learn.\n",
    "            #\n",
    "            # Use any debugging information you think you need.\n",
    "            # For instance :\n",
    "\n",
    "            print(\"Training: Episode = %d, Length = %d, Global step = %d\" % (episode, episode_length, total_steps))\n",
    "            with summary_writer.as_default():\n",
    "                summary.scalar(\"episode length\",episode ,step=episode_length)\n",
    "\n",
    "\n",
    "    # Simple function to visually 'test' a policy\n",
    "    def playPolicy(self):\n",
    "\n",
    "        done = False\n",
    "        steps = 0\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # we assume the CartPole task to be solved if the pole remains upright for 200 steps\n",
    "        while not done and steps < 200: \n",
    "            self.env.render()\n",
    "            action = np.argmax(self.target_model.predict(np.array([state]))[0])\n",
    "            state, _, done, _ = self.env.step(action)\n",
    "            steps += 1\n",
    "\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Training: Episode = 0, Length = 38, Global step = 38\n",
      "Training: Episode = 1, Length = 66, Global step = 104\n",
      "Training: Episode = 2, Length = 10, Global step = 114\n",
      "Training: Episode = 3, Length = 35, Global step = 149\n",
      "Training: Episode = 4, Length = 22, Global step = 171\n",
      "Training: Episode = 5, Length = 36, Global step = 207\n",
      "Training: Episode = 6, Length = 23, Global step = 230\n",
      "Training: Episode = 7, Length = 13, Global step = 243\n",
      "Training: Episode = 8, Length = 19, Global step = 262\n",
      "Training: Episode = 9, Length = 26, Global step = 288\n",
      "Training: Episode = 10, Length = 11, Global step = 299\n",
      "Training: Episode = 11, Length = 41, Global step = 340\n",
      "Training: Episode = 12, Length = 27, Global step = 367\n",
      "Training: Episode = 13, Length = 17, Global step = 384\n",
      "Training: Episode = 14, Length = 18, Global step = 402\n",
      "Training: Episode = 15, Length = 16, Global step = 418\n",
      "Training: Episode = 16, Length = 45, Global step = 463\n",
      "Training: Episode = 17, Length = 18, Global step = 481\n",
      "Training: Episode = 18, Length = 33, Global step = 514\n",
      "Training: Episode = 19, Length = 49, Global step = 563\n",
      "Training: Episode = 20, Length = 54, Global step = 617\n",
      "Training: Episode = 21, Length = 14, Global step = 631\n",
      "Training: Episode = 22, Length = 18, Global step = 649\n",
      "Training: Episode = 23, Length = 9, Global step = 658\n",
      "Training: Episode = 24, Length = 19, Global step = 677\n",
      "Training: Episode = 25, Length = 15, Global step = 692\n",
      "Training: Episode = 26, Length = 36, Global step = 728\n",
      "Training: Episode = 27, Length = 21, Global step = 749\n",
      "Training: Episode = 28, Length = 18, Global step = 767\n",
      "Training: Episode = 29, Length = 31, Global step = 798\n",
      "Training: Episode = 30, Length = 15, Global step = 813\n",
      "Training: Episode = 31, Length = 14, Global step = 827\n",
      "Training: Episode = 32, Length = 13, Global step = 840\n",
      "Training: Episode = 33, Length = 12, Global step = 852\n",
      "Training: Episode = 34, Length = 12, Global step = 864\n",
      "Training: Episode = 35, Length = 12, Global step = 876\n",
      "Training: Episode = 36, Length = 22, Global step = 898\n",
      "Training: Episode = 37, Length = 25, Global step = 923\n",
      "Training: Episode = 38, Length = 16, Global step = 939\n",
      "Training: Episode = 39, Length = 13, Global step = 952\n",
      "Training: Episode = 40, Length = 20, Global step = 972\n",
      "Training: Episode = 41, Length = 17, Global step = 989\n",
      "Training: Episode = 42, Length = 13, Global step = 1002\n",
      "Training: Episode = 43, Length = 27, Global step = 1029\n",
      "Training: Episode = 44, Length = 21, Global step = 1050\n",
      "Training: Episode = 45, Length = 27, Global step = 1077\n",
      "Training: Episode = 46, Length = 18, Global step = 1095\n",
      "Training: Episode = 47, Length = 24, Global step = 1119\n",
      "Training: Episode = 48, Length = 10, Global step = 1129\n",
      "Training: Episode = 49, Length = 21, Global step = 1150\n",
      "Training: Episode = 50, Length = 13, Global step = 1163\n",
      "Training: Episode = 51, Length = 19, Global step = 1182\n",
      "Training: Episode = 52, Length = 41, Global step = 1223\n",
      "Training: Episode = 53, Length = 20, Global step = 1243\n",
      "Training: Episode = 54, Length = 11, Global step = 1254\n",
      "Training: Episode = 55, Length = 24, Global step = 1278\n",
      "Training: Episode = 56, Length = 19, Global step = 1297\n",
      "Training: Episode = 57, Length = 14, Global step = 1311\n",
      "Training: Episode = 58, Length = 40, Global step = 1351\n",
      "Training: Episode = 59, Length = 43, Global step = 1394\n",
      "Training: Episode = 60, Length = 14, Global step = 1408\n",
      "Training: Episode = 61, Length = 16, Global step = 1424\n",
      "Training: Episode = 62, Length = 15, Global step = 1439\n",
      "Training: Episode = 63, Length = 26, Global step = 1465\n",
      "Training: Episode = 64, Length = 21, Global step = 1486\n",
      "Training: Episode = 65, Length = 80, Global step = 1566\n",
      "Training: Episode = 66, Length = 22, Global step = 1588\n",
      "Training: Episode = 67, Length = 14, Global step = 1602\n",
      "Training: Episode = 68, Length = 20, Global step = 1622\n",
      "Training: Episode = 69, Length = 20, Global step = 1642\n",
      "Training: Episode = 70, Length = 34, Global step = 1676\n",
      "Training: Episode = 71, Length = 15, Global step = 1691\n",
      "Training: Episode = 72, Length = 17, Global step = 1708\n",
      "Training: Episode = 73, Length = 21, Global step = 1729\n",
      "Training: Episode = 74, Length = 29, Global step = 1758\n",
      "Training: Episode = 75, Length = 18, Global step = 1776\n",
      "Training: Episode = 76, Length = 11, Global step = 1787\n",
      "Training: Episode = 77, Length = 12, Global step = 1799\n",
      "Training: Episode = 78, Length = 12, Global step = 1811\n",
      "Training: Episode = 79, Length = 20, Global step = 1831\n",
      "Training: Episode = 80, Length = 20, Global step = 1851\n",
      "Training: Episode = 81, Length = 26, Global step = 1877\n",
      "Training: Episode = 82, Length = 28, Global step = 1905\n",
      "Training: Episode = 83, Length = 10, Global step = 1915\n",
      "Training: Episode = 84, Length = 18, Global step = 1933\n",
      "Training: Episode = 85, Length = 41, Global step = 1974\n",
      "Training: Episode = 86, Length = 34, Global step = 2008\n",
      "Training: Episode = 87, Length = 14, Global step = 2022\n",
      "Training: Episode = 88, Length = 15, Global step = 2037\n",
      "Training: Episode = 89, Length = 21, Global step = 2058\n",
      "Training: Episode = 90, Length = 23, Global step = 2081\n",
      "Training: Episode = 91, Length = 17, Global step = 2098\n",
      "Training: Episode = 92, Length = 13, Global step = 2111\n",
      "Training: Episode = 93, Length = 39, Global step = 2150\n",
      "Training: Episode = 94, Length = 26, Global step = 2176\n",
      "Training: Episode = 95, Length = 51, Global step = 2227\n",
      "Training: Episode = 96, Length = 11, Global step = 2238\n",
      "Training: Episode = 97, Length = 32, Global step = 2270\n",
      "Training: Episode = 98, Length = 21, Global step = 2291\n",
      "Training: Episode = 99, Length = 19, Global step = 2310\n",
      "Training: Episode = 100, Length = 25, Global step = 2335\n",
      "Training: Episode = 101, Length = 14, Global step = 2349\n",
      "Training: Episode = 102, Length = 11, Global step = 2360\n",
      "Training: Episode = 103, Length = 8, Global step = 2368\n",
      "Training: Episode = 104, Length = 40, Global step = 2408\n",
      "Training: Episode = 105, Length = 30, Global step = 2438\n",
      "Training: Episode = 106, Length = 16, Global step = 2454\n",
      "Training: Episode = 107, Length = 21, Global step = 2475\n",
      "Training: Episode = 108, Length = 40, Global step = 2515\n",
      "Training: Episode = 109, Length = 14, Global step = 2529\n",
      "Training: Episode = 110, Length = 27, Global step = 2556\n",
      "Training: Episode = 111, Length = 13, Global step = 2569\n",
      "Training: Episode = 112, Length = 11, Global step = 2580\n",
      "Training: Episode = 113, Length = 13, Global step = 2593\n",
      "Training: Episode = 114, Length = 25, Global step = 2618\n",
      "Training: Episode = 115, Length = 15, Global step = 2633\n",
      "Training: Episode = 116, Length = 12, Global step = 2645\n",
      "Training: Episode = 117, Length = 11, Global step = 2656\n",
      "Training: Episode = 118, Length = 36, Global step = 2692\n",
      "Training: Episode = 119, Length = 44, Global step = 2736\n",
      "Training: Episode = 120, Length = 22, Global step = 2758\n",
      "Training: Episode = 121, Length = 14, Global step = 2772\n",
      "Training: Episode = 122, Length = 35, Global step = 2807\n",
      "Training: Episode = 123, Length = 10, Global step = 2817\n",
      "Training: Episode = 124, Length = 19, Global step = 2836\n",
      "Training: Episode = 125, Length = 40, Global step = 2876\n",
      "Training: Episode = 126, Length = 19, Global step = 2895\n",
      "Training: Episode = 127, Length = 16, Global step = 2911\n",
      "Training: Episode = 128, Length = 18, Global step = 2929\n",
      "Training: Episode = 129, Length = 11, Global step = 2940\n",
      "Training: Episode = 130, Length = 18, Global step = 2958\n",
      "Training: Episode = 131, Length = 10, Global step = 2968\n",
      "Training: Episode = 132, Length = 17, Global step = 2985\n",
      "Training: Episode = 133, Length = 12, Global step = 2997\n",
      "Training: Episode = 134, Length = 16, Global step = 3013\n",
      "Training: Episode = 135, Length = 31, Global step = 3044\n",
      "Training: Episode = 136, Length = 22, Global step = 3066\n",
      "Training: Episode = 137, Length = 41, Global step = 3107\n",
      "Training: Episode = 138, Length = 14, Global step = 3121\n",
      "Training: Episode = 139, Length = 35, Global step = 3156\n",
      "Training: Episode = 140, Length = 28, Global step = 3184\n",
      "Training: Episode = 141, Length = 37, Global step = 3221\n",
      "Training: Episode = 142, Length = 21, Global step = 3242\n",
      "Training: Episode = 143, Length = 15, Global step = 3257\n",
      "Training: Episode = 144, Length = 13, Global step = 3270\n",
      "Training: Episode = 145, Length = 49, Global step = 3319\n",
      "Training: Episode = 146, Length = 18, Global step = 3337\n",
      "Training: Episode = 147, Length = 18, Global step = 3355\n",
      "Training: Episode = 148, Length = 29, Global step = 3384\n",
      "Training: Episode = 149, Length = 13, Global step = 3397\n",
      "Training: Episode = 150, Length = 19, Global step = 3416\n",
      "Training: Episode = 151, Length = 16, Global step = 3432\n",
      "Training: Episode = 152, Length = 13, Global step = 3445\n",
      "Training: Episode = 153, Length = 20, Global step = 3465\n",
      "Training: Episode = 154, Length = 20, Global step = 3485\n",
      "Training: Episode = 155, Length = 12, Global step = 3497\n",
      "Training: Episode = 156, Length = 12, Global step = 3509\n",
      "Training: Episode = 157, Length = 15, Global step = 3524\n",
      "Training: Episode = 158, Length = 26, Global step = 3550\n",
      "Training: Episode = 159, Length = 20, Global step = 3570\n",
      "Training: Episode = 160, Length = 27, Global step = 3597\n",
      "Training: Episode = 161, Length = 55, Global step = 3652\n",
      "Training: Episode = 162, Length = 29, Global step = 3681\n",
      "Training: Episode = 163, Length = 19, Global step = 3700\n",
      "Training: Episode = 164, Length = 13, Global step = 3713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Episode = 165, Length = 12, Global step = 3725\n",
      "Training: Episode = 166, Length = 89, Global step = 3814\n",
      "Training: Episode = 167, Length = 17, Global step = 3831\n",
      "Training: Episode = 168, Length = 18, Global step = 3849\n",
      "Training: Episode = 169, Length = 15, Global step = 3864\n",
      "Training: Episode = 170, Length = 42, Global step = 3906\n",
      "Training: Episode = 171, Length = 31, Global step = 3937\n",
      "Training: Episode = 172, Length = 19, Global step = 3956\n",
      "Training: Episode = 173, Length = 13, Global step = 3969\n",
      "Training: Episode = 174, Length = 13, Global step = 3982\n",
      "Training: Episode = 175, Length = 12, Global step = 3994\n",
      "Training: Episode = 176, Length = 13, Global step = 4007\n",
      "Training: Episode = 177, Length = 22, Global step = 4029\n",
      "Training: Episode = 178, Length = 22, Global step = 4051\n",
      "Training: Episode = 179, Length = 12, Global step = 4063\n",
      "Training: Episode = 180, Length = 34, Global step = 4097\n",
      "Training: Episode = 181, Length = 14, Global step = 4111\n",
      "Training: Episode = 182, Length = 26, Global step = 4137\n",
      "Training: Episode = 183, Length = 56, Global step = 4193\n",
      "Training: Episode = 184, Length = 76, Global step = 4269\n",
      "Training: Episode = 185, Length = 22, Global step = 4291\n",
      "Training: Episode = 186, Length = 39, Global step = 4330\n",
      "Training: Episode = 187, Length = 30, Global step = 4360\n",
      "Training: Episode = 188, Length = 14, Global step = 4374\n",
      "Training: Episode = 189, Length = 41, Global step = 4415\n",
      "Training: Episode = 190, Length = 39, Global step = 4454\n",
      "Training: Episode = 191, Length = 35, Global step = 4489\n",
      "Training: Episode = 192, Length = 11, Global step = 4500\n",
      "Training: Episode = 193, Length = 13, Global step = 4513\n",
      "Training: Episode = 194, Length = 10, Global step = 4523\n",
      "Training: Episode = 195, Length = 13, Global step = 4536\n",
      "Training: Episode = 196, Length = 18, Global step = 4554\n",
      "Training: Episode = 197, Length = 19, Global step = 4573\n",
      "Training: Episode = 198, Length = 20, Global step = 4593\n",
      "Training: Episode = 199, Length = 12, Global step = 4605\n",
      "Training: Episode = 200, Length = 28, Global step = 4633\n",
      "Training: Episode = 201, Length = 39, Global step = 4672\n",
      "Training: Episode = 202, Length = 24, Global step = 4696\n",
      "Training: Episode = 203, Length = 9, Global step = 4705\n",
      "Training: Episode = 204, Length = 20, Global step = 4725\n",
      "Training: Episode = 205, Length = 15, Global step = 4740\n",
      "Training: Episode = 206, Length = 24, Global step = 4764\n",
      "Training: Episode = 207, Length = 16, Global step = 4780\n",
      "Training: Episode = 208, Length = 33, Global step = 4813\n",
      "Training: Episode = 209, Length = 22, Global step = 4835\n",
      "Training: Episode = 210, Length = 25, Global step = 4860\n",
      "Training: Episode = 211, Length = 41, Global step = 4901\n",
      "Training: Episode = 212, Length = 45, Global step = 4946\n",
      "Training: Episode = 213, Length = 13, Global step = 4959\n",
      "Training: Episode = 214, Length = 12, Global step = 4971\n",
      "Training: Episode = 215, Length = 12, Global step = 4983\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:886 __call__\n        self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:158 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential_1 expects 1 inputs, but it received 10 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 4) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b102fc773b03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nStarting training...\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nFinished training...\\nCheck out some demonstrations\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-af8aa5e8cd4c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, episodes_num)\u001b[0m\n\u001b[0;32m    191\u001b[0m                         \u001b[0mbatch_targets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPSILON_MIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    625\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 506\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2446\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2447\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:886 __call__\n        self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:158 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential_1 expects 1 inputs, but it received 10 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 4) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize the model\n",
    "dqn = DQN('CartPole-v0')\n",
    "dqn.initialize_network()\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "dqn.train()\n",
    "print(\"\\nFinished training...\\nCheck out some demonstrations\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned behaviour for a few episodes\n",
    "results = []\n",
    "for i in range(50):\n",
    "    episode_length = dqn.playPolicy()\n",
    "    print(\"Test steps = \", episode_length)\n",
    "    results.append(episode_length)\n",
    "print(\"Mean steps = \", sum(results) / len(results))\t\n",
    "\n",
    "print(\"\\nFinished.\")\n",
    "print(\"\\nCiao, and hasta la vista...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5c3067faabd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "b = np.array([])\n",
    "b.append([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
